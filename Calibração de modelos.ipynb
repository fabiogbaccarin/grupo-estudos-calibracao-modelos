{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "51787336",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Calibração de modelos de classificação\n",
    "Esse notebook mostra como podemos calibrar as previsões de modelos de classificação para que elas reflitam mais precisamente a probabilidade do evento ocorrer. Calibrar modelos de classificação é muito importante para sua correta aplicação em contextos práticos, pois facilita a sua interpretação por humanos em vários contextos. Além disso, a calibração permite o uso direto dos modelos em cálculos de negócio, como a perda esperada de crédito.\n",
    "\n",
    "Os modelos não foram criados iguais no que se refere à calibração das previsões: modelos de árvore e Naive Bayes são notórios por gerar previsões descalibradas[[1]](https://scikit-learn.org/stable/modules/calibration.html#calibration). Usaremos as ferramentas do Scikit-Learn para resolver esse problema, usando dados simulados."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "591b6633",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc9471e4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# bibliotecas\n",
    "import pandas as pd\n",
    "import functools\n",
    "from scipy import stats\n",
    "from sklearn import naive_bayes, tree, calibration, pipeline, metrics, datasets, model_selection, linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6976d46b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(40000, 5)\n",
      "(40000,)\n"
     ]
    }
   ],
   "source": [
    "# gerando dados com dimensão (40_000, 5)\n",
    "X, y = datasets.make_classification(\n",
    "    n_samples=40_000,\n",
    "    n_features=5,\n",
    "    n_informative=5,\n",
    "    n_redundant=0,\n",
    "    class_sep=0.5\n",
    ")\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16f5e1f5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# separando os dados em treinamento e teste\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.5,\n",
    "    stratify=y,\n",
    "    random_state=73\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2539260f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e0d0b8d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"logreg\": linear_model.LogisticRegression(),\n",
    "    \"naive_bayes\": naive_bayes.GaussianNB(),\n",
    "    \"dtc\": tree.DecisionTreeClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0cb05c4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# rodando os modelos\n",
    "for _, m in models.items():\n",
    "    m.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6438fb9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para medir o desempenho de modelos no que se refere à calibração de probabilidade, temos duas opções no Sci-kit Learn: o [Score de Brier](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.brier_score_loss.html#sklearn.metrics.brier_score_loss) e a [Log loss](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.log_loss.html#sklearn.metrics.log_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "338d42e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg': {'brier': 0.17328984236755587,\n",
       "  'logloss': 0.5207789447056256,\n",
       "  'auc': 0.8191846947961736},\n",
       " 'naive_bayes': {'brier': 0.17593805903884294,\n",
       "  'logloss': 0.52639625476901,\n",
       "  'auc': 0.8179033844758462},\n",
       " 'dtc': {'brier': 0.16095,\n",
       "  'logloss': 5.801226012978405,\n",
       "  'auc': 0.8390519847629963}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance = {\n",
    "    model_name: {\n",
    "        \"brier\": metrics.brier_score_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        \"logloss\": metrics.log_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        # para mostrar que a calibração não altera a eficácia preditiva do modelo\n",
    "        \"auc\": metrics.roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    for model_name, model in models.items()\n",
    "}\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bad449",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Calibrando classificadores usando o Sci-kit Learn\n",
    "A calibração de classificadores no Sci-kit Learn é implantada pela classe [CalibratedClassifierCV](https://scikit-learn.org/stable/modules/generated/sklearn.calibration.CalibratedClassifierCV.html#sklearn-calibration-calibratedclassifiercv). Essa classe por padrão faz cross-validation e estima uma série de estimadores para então calibrar. Não é isso que queremos. Queremos treinar um único estimador com *todos* os dados e então calibrar esse classificador. Para isso, simplesmente passamos o parâmetro `ensemble=False`.\n",
    "\n",
    "### Opção 1: Platt scaling\n",
    "A primeira opção de calibração é chamada [Platt scaling](https://en.wikipedia.org/wiki/Platt_scaling). Esse método consiste em rodar uma regressão logística com as previsões do modelo, calibrando a probabilidade no processo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14b28eb0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def make_calibrated_classifier(model, **kwargs):\n",
    "    return calibration.CalibratedClassifierCV(\n",
    "        model,\n",
    "        # partições de cross-validation\n",
    "        cv=5,\n",
    "        # apenas calibrar por validação cruzada\n",
    "        ensemble=False,\n",
    "        # usa todas as CPUs do computador para ser mais rápido\n",
    "        n_jobs=-1,\n",
    "        **kwargs\n",
    "    )\n",
    "\n",
    "# um partial é uma função que recebe um ou mais argumentos fixos\n",
    "platt_scaling = functools.partial(make_calibrated_classifier, method=\"sigmoid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1de4126a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calibrated_models_platt = {\n",
    "    model_name: platt_scaling(model).fit(X_train, y_train)\n",
    "    for model_name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d963e78",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg': {'brier': 0.17329114858312886,\n",
       "  'logloss': 0.5207803549972551,\n",
       "  'auc': 0.8191846947961736},\n",
       " 'naive_bayes': {'brier': 0.176030373460807,\n",
       "  'logloss': 0.5269003783488727,\n",
       "  'auc': 0.8179033844758462},\n",
       " 'dtc': {'brier': 0.13542727330446255,\n",
       "  'logloss': 0.44218368722127344,\n",
       "  'auc': 0.8385030096257524}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_platt = {\n",
    "    model_name: {\n",
    "        \"brier\": metrics.brier_score_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        \"logloss\": metrics.log_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        # para mostrar que a calibração não altera a eficácia preditiva do modelo\n",
    "        \"auc\": metrics.roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    for model_name, model in calibrated_models_platt.items()\n",
    "}\n",
    "performance_platt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a611e1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Opção 2: regressão isotônica\n",
    "Outra opção é estimar uma [Regressão isotônica](https://en.wikipedia.org/wiki/Isotonic_regression) com as previsões do modelo. Mas **atenção**: esse método não é paramétrico, ou seja, precisa mais dados para não gerar overfitting. O Sci-kit Learn recomenda pelo menos 1000 linhas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33201aaf",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "isotonic_scaling = functools.partial(make_calibrated_classifier, method=\"isotonic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27989358",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calibrated_models_isotonic = {\n",
    "    model_name: isotonic_scaling(model).fit(X_train, y_train)\n",
    "    for model_name, model in models.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4296e5f8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'logreg': {'brier': 0.17329114858312886,\n",
       "  'logloss': 0.5207803549972551,\n",
       "  'auc': 0.8191846947961736},\n",
       " 'naive_bayes': {'brier': 0.176030373460807,\n",
       "  'logloss': 0.5269003783488727,\n",
       "  'auc': 0.8179033844758462},\n",
       " 'dtc': {'brier': 0.13542727330446255,\n",
       "  'logloss': 0.44218368722127344,\n",
       "  'auc': 0.8385030096257524}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "performance_isotonic = {\n",
    "    model_name: {\n",
    "        \"brier\": metrics.brier_score_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        \"logloss\": metrics.log_loss(y_test, model.predict_proba(X_test)[:, 1]),\n",
    "        # para mostrar que a calibração não altera a eficácia preditiva do modelo\n",
    "        \"auc\": metrics.roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "    }\n",
    "    for model_name, model in calibrated_models_platt.items()\n",
    "}\n",
    "performance_isotonic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d36b2e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "16c4c2b0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>brier</th>\n",
       "      <th>logloss</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>method</th>\n",
       "      <th>model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">uncalibrated</th>\n",
       "      <th>logreg</th>\n",
       "      <td>0.173290</td>\n",
       "      <td>0.520779</td>\n",
       "      <td>0.819185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayes</th>\n",
       "      <td>0.175938</td>\n",
       "      <td>0.526396</td>\n",
       "      <td>0.817903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtc</th>\n",
       "      <td>0.160950</td>\n",
       "      <td>5.801226</td>\n",
       "      <td>0.839052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">platt</th>\n",
       "      <th>logreg</th>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.520780</td>\n",
       "      <td>0.819185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayes</th>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.817903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtc</th>\n",
       "      <td>0.135427</td>\n",
       "      <td>0.442184</td>\n",
       "      <td>0.838503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">isotonic</th>\n",
       "      <th>logreg</th>\n",
       "      <td>0.173291</td>\n",
       "      <td>0.520780</td>\n",
       "      <td>0.819185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>naive_bayes</th>\n",
       "      <td>0.176030</td>\n",
       "      <td>0.526900</td>\n",
       "      <td>0.817903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dtc</th>\n",
       "      <td>0.135427</td>\n",
       "      <td>0.442184</td>\n",
       "      <td>0.838503</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             brier   logloss       auc\n",
       "method       model                                    \n",
       "uncalibrated logreg       0.173290  0.520779  0.819185\n",
       "             naive_bayes  0.175938  0.526396  0.817903\n",
       "             dtc          0.160950  5.801226  0.839052\n",
       "platt        logreg       0.173291  0.520780  0.819185\n",
       "             naive_bayes  0.176030  0.526900  0.817903\n",
       "             dtc          0.135427  0.442184  0.838503\n",
       "isotonic     logreg       0.173291  0.520780  0.819185\n",
       "             naive_bayes  0.176030  0.526900  0.817903\n",
       "             dtc          0.135427  0.442184  0.838503"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.concat([\n",
    "    pd.DataFrame.from_dict(performance, orient=\"index\").assign(method=\"uncalibrated\"),\n",
    "    pd.DataFrame.from_dict(performance_platt, orient=\"index\").assign(method=\"platt\"),\n",
    "    pd.DataFrame.from_dict(performance_isotonic, orient=\"index\").assign(method=\"isotonic\"),\n",
    "]).reset_index().rename(columns={\"index\": \"model\"}).set_index([\"method\", \"model\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}